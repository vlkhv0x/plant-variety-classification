"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—Ü–µ–Ω–∫–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
"""

import os
import argparse
import json
import numpy as np
import pandas as pd
from pathlib import Path
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import classification_report, confusion_matrix
import tensorflow as tf
from tensorflow import keras

from data_preprocessing import SeedDataPreprocessor
from utils import plot_confusion_matrix, plot_sample_predictions


def parse_args():
    """–ü–∞—Ä—Å–∏–Ω–≥ –∞—Ä–≥—É–º–µ–Ω—Ç–æ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏"""
    parser = argparse.ArgumentParser(description='–û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ —Å–µ–º—è–Ω')
    
    parser.add_argument('--model_path', type=str, default='models/best_model.h5',
                        help='–ü—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏')
    parser.add_argument('--data_dir', type=str, default='data/raw/seeds',
                        help='–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å –¥–∞–Ω–Ω—ã–º–∏')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞')
    parser.add_argument('--img_size', type=int, default=224,
                        help='–†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è')
    
    return parser.parse_args()


def evaluate_model(model, test_generator, class_names):
    """
    –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    
    Args:
        model: –æ–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å
        test_generator: –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        class_names: —Å–ø–∏—Å–æ–∫ –∏–º—ë–Ω –∫–ª–∞—Å—Å–æ–≤
        
    Returns:
        results: —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    """
    print("\n" + "=" * 70)
    print("üìä –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –ù–ê –¢–ï–°–¢–û–í–û–ô –í–´–ë–û–†–ö–ï")
    print("=" * 70)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    print("\nüîÆ –ü–æ–ª—É—á–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π...")
    test_generator.reset()
    predictions = model.predict(test_generator, verbose=1)
    y_pred = np.argmax(predictions, axis=1)
    
    # –ò—Å—Ç–∏–Ω–Ω—ã–µ –º–µ—Ç–∫–∏
    y_true = test_generator.classes
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    print("\nüìà –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")
    test_loss, test_accuracy = model.evaluate(test_generator, verbose=0)
    
    # Classification report
    report = classification_report(
        y_true, y_pred,
        target_names=class_names,
        output_dict=True
    )
    
    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred)
    
    # –í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\n" + "=" * 70)
    print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´")
    print("=" * 70)
    
    print(f"\nüéØ –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print(f"   Test Loss: {test_loss:.4f}")
    print(f"   Test Accuracy: {test_accuracy:.4f}")
    
    print(f"\nüìä –£—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:")
    print(f"   Precision (macro): {report['macro avg']['precision']:.4f}")
    print(f"   Recall (macro): {report['macro avg']['recall']:.4f}")
    print(f"   F1-Score (macro): {report['macro avg']['f1-score']:.4f}")
    
    print(f"\nüìã Per-class –º–µ—Ç—Ä–∏–∫–∏:")
    print("-" * 70)
    print(f"{'Class':<20} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}")
    print("-" * 70)
    
    for class_name in class_names:
        metrics = report[class_name]
        print(f"{class_name:<20} {metrics['precision']:<12.4f} "
              f"{metrics['recall']:<12.4f} {metrics['f1-score']:<12.4f} "
              f"{int(metrics['support']):<10}")
    
    print("-" * 70)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    results = {
        'test_loss': float(test_loss),
        'test_accuracy': float(test_accuracy),
        'classification_report': report,
        'confusion_matrix': cm.tolist(),
        'predictions': predictions.tolist(),
        'true_labels': y_true.tolist(),
        'predicted_labels': y_pred.tolist()
    }
    
    return results


def save_classification_report(report, class_names, save_path='reports/classification_report.txt'):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ classification report –≤ —Ç–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª"""
    
    with open(save_path, 'w', encoding='utf-8') as f:
        f.write("=" * 80 + "\n")
        f.write("–û–¢–ß–Å–¢ –ü–û –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –°–û–†–¢–û–í –†–ê–°–¢–ï–ù–ò–ô\n")
        f.write("=" * 80 + "\n\n")
        
        f.write("Per-class –º–µ—Ç—Ä–∏–∫–∏:\n")
        f.write("-" * 80 + "\n")
        f.write(f"{'Class':<25} {'Precision':<12} {'Recall':<12} {'F1-Score':<12} {'Support':<10}\n")
        f.write("-" * 80 + "\n")
        
        for class_name in class_names:
            metrics = report[class_name]
            f.write(f"{class_name:<25} {metrics['precision']:<12.4f} "
                   f"{metrics['recall']:<12.4f} {metrics['f1-score']:<12.4f} "
                   f"{int(metrics['support']):<10}\n")
        
        f.write("-" * 80 + "\n\n")
        
        f.write("–£—Å—Ä–µ–¥–Ω—ë–Ω–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:\n")
        f.write(f"  Accuracy: {report['accuracy']:.4f}\n")
        f.write(f"  Macro avg - Precision: {report['macro avg']['precision']:.4f}\n")
        f.write(f"  Macro avg - Recall: {report['macro avg']['recall']:.4f}\n")
        f.write(f"  Macro avg - F1-Score: {report['macro avg']['f1-score']:.4f}\n")
        f.write(f"  Weighted avg - Precision: {report['weighted avg']['precision']:.4f}\n")
        f.write(f"  Weighted avg - Recall: {report['weighted avg']['recall']:.4f}\n")
        f.write(f"  Weighted avg - F1-Score: {report['weighted avg']['f1-score']:.4f}\n")
    
    print(f"‚úÖ Classification report —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {save_path}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—Ü–µ–Ω–∫–∏"""
    
    args = parse_args()
    
    print("=" * 70)
    print("üå± –û–¶–ï–ù–ö–ê –ú–û–î–ï–õ–ò –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò –°–û–†–¢–û–í –†–ê–°–¢–ï–ù–ò–ô")
    print("=" * 70)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
    print(f"\nüì• –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏: {args.model_path}")
    if not os.path.exists(args.model_path):
        raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {args.model_path}")
    
    model = keras.models.load_model(args.model_path)
    print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    config_path = 'models/config.json'
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = json.load(f)
        class_names = config['class_names']
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {len(class_names)} –∫–ª–∞—Å—Å–æ–≤")
    else:
        print("‚ö†Ô∏è  config.json –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞")
        class_names = None
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    print("\nüìä –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö...")
    preprocessor = SeedDataPreprocessor(
        data_dir=args.data_dir,
        img_size=(args.img_size, args.img_size),
        batch_size=args.batch_size
    )
    
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ—Å—Ç–æ–≤—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä
    image_paths, labels = preprocessor.load_data_paths()
    df = preprocessor.create_dataframe(image_paths, labels)
    train_df, val_df, test_df = preprocessor.split_data(df)
    _, _, test_gen = preprocessor.create_data_generators(train_df, val_df, test_df)
    
    # –ï—Å–ª–∏ class_names –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥–∞, –±–µ—Ä—ë–º –∏–∑ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
    if class_names is None:
        class_indices = test_gen.class_indices
        class_names = list(class_indices.keys())
    
    # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
    results = evaluate_model(model, test_gen, class_names)
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")
    
    # JSON —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏
    with open('reports/evaluation_results.json', 'w') as f:
        # –£–±–∏—Ä–∞–µ–º predictions –¥–ª—è —É–º–µ–Ω—å—à–µ–Ω–∏—è —Ä–∞–∑–º–µ—Ä–∞ —Ñ–∞–π–ª–∞
        results_to_save = results.copy()
        results_to_save.pop('predictions', None)
        json.dump(results_to_save, f, indent=4)
    
    print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: reports/evaluation_results.json")
    
    # Classification report
    save_classification_report(
        results['classification_report'],
        class_names,
        'reports/classification_report.txt'
    )
    
    # Confusion matrix
    plot_confusion_matrix(
        results['confusion_matrix'],
        class_names,
        save_path='reports/confusion_matrix.png'
    )
    
    # –ü—Ä–∏–º–µ—Ä—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    plot_sample_predictions(
        model,
        test_gen,
        class_names,
        num_samples=16,
        save_path='reports/predictions_sample.png'
    )
    
    print("\n" + "=" * 70)
    print("‚úÖ –û–¶–ï–ù–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê")
    print("=" * 70)
    print(f"\nüìÅ –°–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã:")
    print(f"   - reports/evaluation_results.json")
    print(f"   - reports/classification_report.txt")
    print(f"   - reports/confusion_matrix.png")
    print(f"   - reports/predictions_sample.png")
    print("\n" + "=" * 70)


if __name__ == "__main__":
    main()
